# 百面机器学习总结

主要总结一些之前没太注意的地方，本书对于NLP领域的知识我不会太过重视，所以也不会有很多NLP的总结。

## 主要问题


### 总结

1. 数据归一化问题中，通过梯度下降法优化的算法通常是需要归一化的。但对于决策树模型，归一化不适用。这是因为归一化对信息增益比这类指标的求取不会有影响。
2. one-hot编码有一个很重要的点在于它可以使用稀疏向量来节省空间，比如只给模型这个向量的长度以及取值为1的坐标。另外因为one-hot中一个维度对应一个特征的关系，特征选择后可以直接降低one-hot编码的维度。
3. 模型学习来源一般是两个：数据中的信息、人为给的先验信息。先验信息可以是加在模型里、也可以是加在数据里。比如对图像做灰度上的预处理，加强某些地方，就是把先验知识加在了数据里。把觉得重要的地方放在模型里作为attention，那就是把先验知识加在了模型中。
4. 一般过拟合的处理方式是正则化或者数据扩充，数据扩充的诸多方式中除了旋转等扰动外，还可以做颜色变化。比如求每个像素点的Hessian矩阵，再求其特征值$\lambda$与特征向量$\eta$，取得特征值最大的一组，将原图每个像素点施加一个$\lambda\cdot\eta\cdot\alpha$，其中$\alpha$是一个均值为0，方差较小的高斯分布随机数。 


### 为什么树模型的特征不需要归一化？

使用归一化的目的是因为各个特征之间数值大小不同使得它们对**某些模型（通过梯度下降法求解的模型通常是需要归一化的），比如神经网络**的输出和损失计算的影响不同，这样反向传播时会偏向于数值比较大的特征，这样是不合理的。
因为在没有任何先验知识的情况下，各个特征应该一视同仁。但是，树模型是直接利用数据的信息熵来建立的，不需要计算损失函数并反向传播，因此各个特征之间数值的大小不同不会影响
它的建模过程。换句话说，神经网络里数据的数值会直接和权重相乘，进行计算，树模型不会直接使用数值本身，而是使用它出现的概率。

### 为什么需要onehot编码？

首先onehot应用在分类问题对label的编码中，它的特点是使得各个类别之间距离相等。如果不使用onehot而是直接使用0, 1,2这样的数值来代替类别，那么不同类之间的距离就不同了，那么这个问题就
变成了回归的问题。另外因为one-hot中一个维度对应一个特征的关系，特征选择后可以直接降低one-hot编码的维度。

但是需要注意的是，有些分类问题中不同类别之间的距离是不同的，这时直接使用onehot是不妥的，可以在计算损失时按照不同距离之间的比例关系，给不同类别之间赋予不同的权重。

### 解决过拟合的方法：

- 正则化，比如L1正则和L2正则，通常L2用的多。（为什么L1会使网络变稀疏而L2不会？**因为L1是绝对值，求导后变成数值，每次反向传播减去的都是一个具体的数，最后会变成0；而L2是平方，求导后是本身数值的比例如1/2，
每次减去的是一个逐渐变小的数，所以不会变成0。更具体的说：。加入L1正则项后，对带正则项的目标函数求导，正则项部分产生的导数在原点左边部分是−C，在原点右边部分是C，因此，
只要原目标函数的导数绝对值小于C，那么带正则项的目标函数在原点左边部分始终是递减的，在原点右边部分始终是递增的，最小值点自然在原点处。相反，L2正则项在原点处的导数是0，只要原目标
函数在原点处的导数不为0，那么最小值点就不会在原点，所以L2只有减小w绝对值的作用，对解空间的稀疏性没有贡献**）。另外，使用L1正则的线性回归叫岭回归，使用L2正则的回归叫LASSO回归。

- 增大训练数据量。使模型见到的更多了，所以泛化性能更好。

- 数据增强。使用先验信息对数据进行扩充，相当于增大了训练数据量。**需要注意的是，增强的方式必须是和先验信息相符合的，比如数字识别的时候不能翻转，如6会变成9，相当于引入了错误的先验知识**

- BN。
    - BN有效的原因：首先是归一化使得模型每一层的输出值变得稳定，易于训练；其次在归一化过程中使用的均值和标准差是在mini-batch上计算的，它们相对整个数据集的均值标准差，具有一定的
    噪声，这就使得BN有轻微的正则化效果。  
    - BN中r和b的作用：因为我们不知道每一层做归一化是不是一定会更好，也不知需要归一化到什么程度，所以设置这两个参数让模型自己去学到合适的归一化。  
    - 需要注意的是，在batch_size很小时，BN的效果会变得很差。因为均值和标准差的噪声太大了。   
    - BN均值标准差的计算  
    - BN相当于把每一个channel当成一个特征，和GAP的思想类似  
    - **训练时和测试时的不同点：测试时用的均值和方差是全部训练数据的均值和方差**

- GN+WS。BN、GN都是特征图的平滑，WS是对每一个卷积核的权重进行归一化，是对权重的平滑，因此两者可以同时使用。

- mixup、label smoothing等

- 模型集成。训练多个模型，然后求均值或投票。

- 模型简化。剪枝、dropout、知识蒸馏。
    - dropout在只在训练时起作用。Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。  
    - 剪枝。通过剪掉一些层看输出的变化大不大，如果误差在可接受范围内，那么就把它剪掉。
    - 蒸馏。通过训练一个比较小的学生网络，使学生网络达到教师网络的效果。
    
- Early stopping。

#### 

>BN：对其求均值和方差时，将在 N、H、W上操作，而保留通道 C 的维度。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 ...... 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以 N×H×W 而不是单纯除以 N，最后得到的是一个代表这个 batch 第1个通道平均值的数字，而不是一个 H×W 的矩阵）。
GN ：计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。多用于检测分割等占用显存较大的任务。
LN： 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。适合训练数据长度不同的 RNN 模型。
IN ：对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C 维度，也就是说，它只在 channel 内部求均值和标准差。最初用于图像的风格迁移，在生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，因此可以先把图像在 channel 层面归一化，然后再用目标风格图片对应 channel 的均值和标准差“去归一化”，以期获得目标图片的风格。


###  ROC曲线和PR曲线

1. ROC曲线与P-R曲线，本质上都是为了权衡模型在不同阈值下的性能。ROC曲线的直观感受是：希望模型就算误报、也尽量不要漏报（横轴是FPR，纵轴是TPR也就是召回率）。P-R曲线便是precision和recall之间直接的一种trade-off。ROC曲线随着数据集大量负例的增加不会有什么变化，因为ROC的纵轴是召回率，负例的增加是几乎不会影响召回率的，这一点从召回率的公式能看出。P-R曲线由于纵轴是precision，所以会受大量FP的影响。ROC能更好地反映模型本身的好坏，P-R曲线能更直观地反映模型在某数据集上的性能。
2. ROC曲线中，（0,0）点和（1,1）点的解释。当阈值为1，所有的样本都被预测为Negative，TP和FP都为0，所以TPR=FPR=0；当阈值为0时，所有的样本都被预测为Positive，TN和FN为0，所以TPR=FPR=1

3. 分类问题中ROC曲线和PR曲线的对比：

当负样本变多时，FP会增加，Precision会降低，所以PR曲线的变化会比较大；但是对ROC来说负样本不会改变recall的值（也就是TPR的值），所ROC曲线影响不大。所以ROC曲线在各种数据集上是比较
稳定的，因此作为对模型的评价指标更客观。而PR曲线受数据影响比较大，因此对模型的评价不稳定。所以，通常用ROC曲线评价各个模型之间的优劣。


### [逻辑回归推导](https://github.com/Deep-Learning-Studyroom/offer/blob/master/algorithms/%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC.md)

### [SVM的推导](https://github.com/Deep-Learning-Studyroom/offer/blob/master/algorithms/%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC.md)

### SVM中松弛变量和惩罚因子C

松弛变量和惩罚因子是为了把线性可分SVM拓展为线性不可分SVM的。只有被决策面分类错误的点（线性不可分点）才会有松弛变量，然后惩罚因子是对线性不可分点的惩罚。
增大惩罚因子，模型泛化性能变弱，惩罚因子无穷大时，退化为线性可分SVM（硬间隔）；
减少惩罚因子，模型泛化性能变好。

- 并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，或者也可以这么看，所有没离群的点松弛变量都等于0  
- 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远。  
- 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，
最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。当C无穷大时，为了最小化损失函数，只能使松弛变量
无穷小（趋近于0），等价于线性可分SVM。

- SVM中的系数求解：SMO算法（Sequential Minimal Optimization）。每次取其中两个乘子作为变量，其他因子作为常数。将N个解问题，转换成两个变量的求解问题。  

### 决策树

#### 决策树基础

- 决策树的建模过程只有前向，神经网络的建模首先是前向传播，然后根据损失函数的梯度反向传播。

- 决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类

- 完全生长的决策树模型具有简单直观、解释性强的特点

- 一般而言，决策树的生成包含了**特征选择、树的构造、树的剪枝**三个过程

- 建立决策树的关键，即在当前状态下选择哪个属性作为分类依据。根据不同的目标函数，建立决策树主要有一下三种算法。
：
    - ID3  使用**最大信息增益**来选择特征；在每个结点上产生出多叉分支，且每个特征在层级之间不会复用；只能处理离散型变量；只能做分类；ID3对样本特征缺失值比较敏感；
    
    - C4.5  使用**最大信息增益率**来选择特征；在每个结点上产生出多叉分支，且每个特征在层级之间不会复用；可以处理连续性变量，C4.5处理连续型变量时，通过对数据排序之后找到类别不同的分割线作为切分点，
      根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量；只能做分类；
    
    - CART  **基尼系数、回归树使用最小平方误差准则**；每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用；可以处理连续性变量，对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量；可以做分类和回归

- 信息增益、相对熵、KL散度、信息散度是一个东西


#### 决策树预剪枝（其实就是early stopping）

当树的深度大于阈值或准确率不再提高或到达当前结点的样本数量小于某个阈值时，停止树的生长。

预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。但如何准确地估计何时停止树的生长（即上述方法中的深度或阈值），针对不同问
题会有很大差别，需要一定经验判断。且预剪枝存在一定局限性，有欠拟合的风险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能
会有显著上升。

#### 决策树后剪枝

后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样
按照多数投票的原则进行判断。同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。

**相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。**

### PCA算法流程

1. 对样本数据进行中心化  
2. 求样本的协方差矩阵  
3. 对协方差矩阵进行特征值分解，并把特征值从小到大排列  
4. 取特征值前d大的特征值向量，把n维的样本数据分别映射到这d个特征向量的方向上，得到了降维后的d维数据。 

知识点  
-  **PCA的目标是最大化投影方差**。因为我们认为信号具有较大的方差，噪声具有较小的方差，信噪比越大越好。  

- 原数据投影后的方差就是协方差矩阵的特征值。所以我们要找的**最大方差就是协方差矩阵的最大特征值，最佳投影方向就是最大特征值对应的特征向量**  

- PCA是一种**线性降维方法**，可以使用核方法变为核PCA。  

- 也可以从最小平方误差的角度解释PCA。在高维空间中，我们实际上是要找到一个d维超平面，使得数据点到这个超平面的距离平方和最小。  

- PCA是无监督的，因此在分类问题中，最好不要用PCA。  


### 线性判别分析（Linear Discriminant Analysis，LDA）

- LDA也是**线性降维方法**，而且是**有监督的**。  

- LDA中心思想：**最大化类间距离，最小化类内距离**。目标函数=类间距离/类内距离，类内距离等于各个类的方差之和。

- 它对数据的分布做了一些很强的假设，例如，每个类数据都是高斯分布、各个类的协方差相等。尽管这些假设在实际中并不一定完全满足，但LDA已
被证明是非常有效的一种降维方法。主要是因为线性模型对于噪声的鲁棒性比较好，但由于模型简单，表达能力有一定局限性，我们可以通过引入核函数
扩展LDA方法以处理分布较为复杂的数据。


### Kmeans算法流程

1. 数据预处理，比如归一化、异常点处理

2. 随机选取K个中心

3. 定义目标函数：每个点距离类别中心的平方和最小  

4. 从0开始，进行下面的迭代，直接聚类中心不再改变：
    1. 将每一个样本分配给距离最近的簇  
    2. 对每一个簇重新计算中心
    

**伪代码：**

```python
function K-Means(输入数据，中心点个数K)  
    获取输入数据的维度D和个数N  
    随机生成K个D维的点  
    while(算法未收敛)  
        对N个点：计算每个点属于哪一类。（for循环）  
        对于K个中心点：（for循环）  
            1，找出所有属于自己这一类的所有数据点  
            2，把自己的坐标修改为这些数据点的中心点坐标  
    end  
    输出结果：  
end
```

算法收敛条件：代价函数不变化或者说聚类中心不变

### Kmeans算法的优缺点是什么？

- 复杂度：
    - 时间复杂度：O(KNJD), J是迭代次数
    
    - 空间复杂度：O(ND)
    
- 优点：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是接近于线性  

- 缺点：
    - K是超参数，需要调参；  
    - 受初值和离群点的影响每次的结果不稳定；    
    - 结果通常不是全局最优而是局部最优解；  
    - 无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）
    - 样本点只能被划分到单一的类中    

### Kmeans的调优   

- 数据归一化和离群点处理。K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，
所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，
因此使用K均值聚类算法之前通常需要对数据做预处理。   

- 合理选择K值。比如使用手肘法，从小到大设置不同的K值，分别得到对应的损失，取曲线刚开始趋于平稳的点对应的K值。  

- 采用核函数。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。非线性
映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。    

- Kmeans++。对中心点初始化的优化。随机选取聚类中心时，后选择的点与已经选择的所有点越远被选择的概率越大。  

- ISODATA算法。对高维数据K值很难确定问题的优化。思想：当某一类数据点比较少，则删除这一类；当某一类数据点很多，则分裂为两类。也就是说，
在Kmeans的基础上增加**分裂操作**和**合并操作**。  


### 高斯混合模型（GMM）

- 基本思想：高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布（**分布的个数类似于Kmeans里的K，也是超参数**）中生成出来的。
在该假设下，每个单独的分模型都是标准高斯模型，其均值μi和方差Σi是待估计的参数。此外，每个分模型都还有一个参数πi，可以理解为权重或
生成数据的概率。  

- 算法步骤： 

    1.初始随机选择各参数的值（**每个分布三个参数**）  
    
    2. EM迭代：
      
        E步：根据每个分布当前的参数，计算每个点由每个分模型生成的概率  
        
        M步：使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重   
        

我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，获得
每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变
化，或者变化非常小时，便得到了比较合理的一组高斯分布 

### GMM和Kmeans的相同点和联系

相同点：

- 它们都是可用于聚类的算法；

- 都需要指定K值；

- 都是使用EM算法来求解；

- 都往往只能收敛于局部最优。

区别：

GMM相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的
样本点。

GMM算法是生成模型

### 自组织映射神经网络（Self-Organizing Map，SOM）  

作用：可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。

算法思想：

- （1）某些视网膜对特征模式的输入会产生激活。

- （2）在生物神经系统中，还存在着一种侧抑制现象，即一个神经细胞兴奋后，会对周围其他神经细胞产生抑制作用

- 自组织映射神经网络**本质上是一个两层的神经网络**，包含输入层和输出层（竞争层）。**输入层模拟感知外界输入信息的视网膜，输出层模拟做出响应的
大脑皮层。输出层中神经元的个数通常是聚类的个数**。

SOM的学习过程： 

- 训练时采用“竞争学习”的方式，每个输入的样例在**输出层中找到一个和它最匹配的节点，称为激活节点**，也叫winning neuron；

- 紧接着用随机梯度下降法更新激活节点的参数；

- 同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数

- 在迭代结束之后，每个样本所激活的神经元就是它对应的类别。

### 自组织映射神经网络与K均值算法的区别如下：

- K均值算法需要事先定下类的个数，也就是K的值。而自组织映射神经网络则不用，隐藏层中的某些节点可以没有任何输入数据属于它，因此聚类结果
的实际簇数可能会小于神经元的个数。而K均值算法受K值设定的影响要更大一些。  

- K均值算法为每个输入数据找到一个最相似的类后，只更新这个类的参数；自组织映射神经网络则会更新临近的节点。所以，**K均值算法受noise data
的影响比较大，而自组织映射神经网络的准确性可能会比K均值算法低（因为也更新了临近节点）**。   

- 相比较而言，自组织映射神经网络的可视化比较好，而且具有优雅的拓扑关系图。  


### DBSCAN  

- 对象的ε-邻域：一个对象的半径ε的区域。

- 核心对象：一个对象ε邻域内的点数超过阈值。

- 直接密度可达：如果某对象p存在于某核心对象q的ε邻域内，则称对象p是核心对象q直接密度可达的。

- 密度可达：如果p1是p0直接密度可达的，p2是p1直接密度可达的，那么称p0到p2密度可达。

- 密度相连：如果p1是q密度可达的， p2是q密度可达的，那么p1和p2是密度相连。

- 簇：一个基于密度的簇是最大的密度相连对象的集合。

- 噪声：不包含在任何簇中的对象称为噪声

### 损失函数

- 0-1损失

- Hinge损失  


- 交叉熵损失  

- Focal loss 

Focal loss有两个参数： r 和 a， r用来惩罚错的离谱的对象（hard examples），a用来惩罚负样本，所以组合起来，focal loss着重惩罚hard negative examples，这样
在一张图像中，前景对象所占像素点极小，背景像素点极多时，有很好的效果。

- Diceloss 

用在图像分割上，只惩罚label=1的像素点，dice系数的分子是2 * 预测概率 * label（label=1的样本）, 分母是 预测概率的平方 + label的平方。 
dice损失一般是 1-dice系数


- MSELoss

- L1Loss  相比做均值回归的平方损失函数，绝对损失函数对异常点更鲁棒一些。

- SmoothL1Loss   首先解决了L1Loss在0处不可导的问题，其次在靠近0时梯度会逐渐下降，使优化逐渐变得缓慢。



### 优化




### boosting和bagging的区别：

- 解决的问题不一样。boosting是解决基学习器欠拟合的问题，bagging是解决基学习器过拟合的问题。所以，Boosting的基学习器都是弱学习器，而bagging的学习器都是强学习器。

- Bagging的各个基学习器是独立的，可以并行计算，而Boosting是串行的，不可以并行计算。

- Bagging的最终的结果是平均的，Boosting对每一个基学习器权重不同。

- 

### AdaBoost（Adaptive Boosting）训练过程：

刚开始训练时对每一个训练样本赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重。
也就是让学习算法在每次学习以后更注意学错的样本。

基于调整后的样本训练下一个基学习器，如此反复，知道基学习器数量达到事先制定的值T。

将这T个基学习器进行线性加权组合。
		 
指数损失函数


### 最常用的基分类器是决策树，主要有以下3个方面的原因。

（1）决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用
过采样的方法来调整样本权重。

（2）决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。

（3）数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决
策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，
在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，
很好地引入了随机性。

除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型
也比较“不稳定”，而且还可以通过调整神经元数量、连接方式、网络层数、初始
权值等方式引入随机性。



### GBDT的优点：
（1）预测阶段基模型是并行计算的。
（2）它在分布稠密的数据集上，表达能力和泛化能力都很好（拟合能力强和方差小）。
（3）它的基模型是决策树，具有较强可解释性，且不需要对数据归一化等预处理。

### GBDT的缺点：
（1）它的训练过程是串行的，无法并行，速度问题。
（2）在高维稀疏数据集上，不如神经网络。
（3）在文本分类的时候表现不太好。

### GBDT和XGBOOST的区别和联系：
（1）GBDT是一个算法，XGBOOST是GBDT的工程实现
（2）当使用CART作为基模型时，XGBOOST显式地加入了正则项。
（3）GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代
价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
（4）GBDT的基学习器只能是CART，但XGBOOST还可以使用线性分类器等多种类型的基分类器。
（5）GBDT训练时直接使用全部数据集，XGBOOST则使用了类似随机森林的采样策略。
（6）GBDT对缺失值没有进行处理，XGBOOST能自动学习出缺失值的处理策略。


































