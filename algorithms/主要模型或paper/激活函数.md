# 常见激活函数分析

## sigmoid
### 优点
- 优点是啥？可能是输出是(0,1)范围，可以作为概率吧

### 缺点
- sigmoid函数饱和使梯度消失
sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋近于0。这样，几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习。

- sigmoid函数输出不是“零为中心”
一个多层的sigmoid神经网络，如果你的输入x都是正数，那么在反向传播中w的梯度传播到网络的某一处时，权值的变化是要么全正要么全负。

- 指数函数的计算是比较消耗计算资源的。

## tanh

### 优点
- 解决了sigmoid函数不是“零为中心”的问题

### 缺点
- 依然有sigmoid函数过饱和的问题

- 依然有指数运算

## ReLU
### 优点
- 解决了梯度消失的问题，至少在正区间里，神经元不会饱和
- 由于ReLU线性、非饱和的形式，在SGD中能够快速收敛
- 计算速度要快很多。ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快

### 缺点
- ReLU的输出不是“零为中心”(Not zero-centered output)
- 随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。这种神经元的死亡是不可逆转的死亡
解释：训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。因为：ReLU的导数在x>0的时候是1，在x<=0的时候是0。如果x<=0，那么ReLU的输出是0，那么反向传播中梯度也是0，权重就不会被更新，导致神经元不再学习。

也就是说，这个ReLU激活函数在训练中将不可逆转的死亡，导致了训练数据多样化的丢失。在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活。所以，设置一个合适的较小的学习率，会降低这种情况的发生。为了解决神经元节点死亡的情况，有人提出了Leaky ReLU、P-ReLu、R-ReLU、ELU等激活函数。

## LeakyReLU
### 优点
- 神经元不会出现死亡的现象
- 对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。
- 由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。

### 缺点
- Leaky ReLU函数中的α，需要通过先验知识人工赋值。

## PReLU


## RReLU
Randomized Leaky ReLU  
RReLU是Leaky ReLU的random版本，在训练过程中，α是从一个高斯分布中随机出来的，然后再测试过程中进行修正。在测试阶段，把训练过程中所有的取个平均值。

## ELU
ELU的英文全称是“Exponential Linear Units”，中文全称是“指数线性单元”。它试图将激活函数的输出平均值接近零，从而加快学习速度。同时，它还能通过正值的标识来避免梯度消失的问题。根据一些研究显示，ELU分类精确度是高于ReLU的。

### 优点

- ELU包含了ReLU的所有优点。

- 神经元不会出现死亡的情况。

- ELU激活函数的输出均值是接近于零的。

### 缺点

- 计算的时候是需要计算指数的，计算效率低的问题。

## ReLU6


## Maxout

Maxout “Neuron” 是由Goodfellow等人在2013年提出的一种很有特点的神经元，它的激活函数、计算的变量、计算方式和普通的神经元完全不同，并有两组权重。先得到两个超平面，再进行最大值计算。激活函数是对ReLU和Leaky ReLU的一般化归纳，没有ReLU函数的缺点，不会出现激活函数饱和神经元死亡的情况。Maxout出现在ICML2013上，作者Goodfellow将maxout和dropout结合，称在MNIST，CIFAR-10，CIFAR-100，SVHN这4个数据集上都取得了start-of-art的识别率。

ReLU和Leaky ReLU都是它的一个变形。比如的时候，就是ReLU。Maxout的拟合能力非常强，它可以拟合任意的凸函数。Goodfellow在论文中从数学的角度上也证明了这个结论，只需要2个Maxout节点就可以拟合任意的凸函数，前提是“隐含层”节点的个数足够多。

### 优点

- Maxout具有ReLU的所有优点，线性、不饱和性。

- 同时没有ReLU的一些缺点。如：神经元的死亡。

### 缺点

### 从这个激活函数的公式可以看出，每个neuron将有两组w，那么参数就增加了一倍。这就导致了整体参数的数量激增。
